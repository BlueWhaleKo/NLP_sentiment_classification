{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# data를 읽어옴\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls\n",
    "\n",
    "# 단어에 대한 idx 부여\n",
    "def convert_word_to_idx(sents):\n",
    "    for sent in sents:\n",
    "        yield [w2i_dict[word] for word in sent.split(' ')]\n",
    "    return\n",
    "\n",
    "\n",
    "# Sequence Length를 맞추기 위한 padding\n",
    "def add_padding(sents, max_len):\n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(sent)< max_len:\n",
    "            sents[i] += [pad] * (max_len - len(sent))\n",
    "    \n",
    "        elif len(sent) > max_len:\n",
    "            sents[i] = sent[:max_len]\n",
    "    \n",
    "    return sents\n",
    "\n",
    "# torch Variable로 변환\n",
    "def convert_to_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_dict = defaultdict(lambda : len(w2i_dict))\n",
    "pad = w2i_dict['<PAD>']\n",
    "\n",
    "# 데이터 불러오기\n",
    "train_txt_ls, train_label_ls = read_txt('ratings_train.txt')\n",
    "test_txt_ls, test_label_ls = read_txt('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_txt_ls), len(test_txt_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2i_ls = list(convert_word_to_idx(train_txt_ls))\n",
    "test_w2i_ls = list(convert_word_to_idx(test_txt_ls))\n",
    "\n",
    "i2w_dict = {val : key for key, val in w2i_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아\n",
      "더빙..\n",
      "진짜\n",
      "짜증나네요\n",
      "목소리\n"
     ]
    }
   ],
   "source": [
    "for w2i in train_w2i_ls[0]:\n",
    "    print(i2w_dict[w2i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 30\n",
    "\n",
    "x_train = convert_to_variable(add_padding(train_w2i_ls, max_sequence_length))\n",
    "x_test = convert_to_variable(add_padding(test_w2i_ls, max_sequence_length))\n",
    "\n",
    "y_train = convert_to_variable(train_label_ls).float()\n",
    "y_test = convert_to_variable(test_label_ls).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train된 Embedding은 사용하지 않았습니다.\n",
    "\n",
    "모든 embedding은 랜덤으로 초기화된 상태로 학습을 진행하였습니다. (non-static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self,w2i_dict, n_words, embed_size, hid_size, drop_rate, kernel_size_ls, num_filter, n_category):\n",
    "        super(CNN_text, self).__init__()\n",
    "        self.w2i_dict = w2i_dict\n",
    "        self.padding_index = w2i_dict['<PAD>']\n",
    "        self.embed_size = embed_size\n",
    "        self.hid_size = hid_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_filter = num_filter\n",
    "        self.kernel_size_ls = kernel_size_ls\n",
    "        self.num_kernel = len(kernel_size_ls)\n",
    "        self.n_category = n_category\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_words, \n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=self.padding_index\n",
    "        )\n",
    "        \n",
    "        # kernel size는 (n-gram, embed_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), \n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, n_category),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x) # [batch_size, max_length, embed_size]\n",
    "        embed.unsqueeze_(1)  # [batch_size, 1, max_length, embed_size]\n",
    "        \n",
    "        # convolution\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        \n",
    "        # max_pool\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "            \n",
    "        # dropout\n",
    "        dropouted = [F.dropout(pool, self.drop_rate) for pool in pooled]\n",
    "        \n",
    "        # concatenate\n",
    "        concated = torch.cat(pooled, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        logit = self.lin(concated)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'w2i_dict' : w2i_dict,\n",
    "    'n_words' : len(w2i_dict),\n",
    "    'embed_size' : 128,\n",
    "    'hid_size' : 128,\n",
    "    'drop_rate' : 0.5,\n",
    "    'kernel_size_ls' : [2,3,4,5],\n",
    "    'num_filter' : 32,\n",
    "    'n_category' : 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_text(\n",
       "  (embedding): Embedding(450543, 128, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 32, kernel_size=(2, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 32, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 32, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (3): Conv2d(1, 32, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 10.4093115234375,  accuracy :0.554\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 9.951091943359375,  accuracy :0.622\n",
      "=================================================================================================\n",
      "Train epoch : 3,  loss : 9.330300244140625,  accuracy :0.664\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 8.5607712890625,  accuracy :0.716\n",
      "=================================================================================================\n",
      "Train epoch : 5,  loss : 7.69082138671875,  accuracy :0.753\n",
      "=================================================================================================\n",
      "Train epoch : 6,  loss : 6.7921904296875,  accuracy :0.795\n",
      "=================================================================================================\n",
      "Train epoch : 7,  loss : 5.874925,  accuracy :0.828\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 4.9870984375,  accuracy :0.857\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lr = 0.001\n",
    "batch_size = 10000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        \n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        predict = F.softmax(scores).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test)\n",
    "        predict = F.softmax(scores).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / y_test.size(0)\n",
    "        loss = criterion(scores, y_test.long())\n",
    "        \n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item()/y_test.size(0), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
