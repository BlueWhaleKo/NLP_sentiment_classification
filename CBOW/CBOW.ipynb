{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "x_train, y_train = read_txt('../ratings_train.txt')\n",
    "x_test, y_test = read_txt('../ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비어있는 리뷰 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_review(X, Y):\n",
    "    empty_idx_ls = []\n",
    "    \n",
    "    for idx, review in enumerate(X):\n",
    "        if len(review) == 0:\n",
    "            empty_idx_ls.append(idx)\n",
    "    \n",
    "    # idx 값이 큰 것부터 제거 (앞으로 밀리는 것을 방지)\n",
    "    empty_idx_ls = sorted(empty_idx_ls, reverse = True)\n",
    "    \n",
    "    for empty_idx in empty_idx_ls:\n",
    "        del X[empty_idx], Y[empty_idx]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = remove_empty_review(x_train, y_train)\n",
    "x_test, y_test = remove_empty_review(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "embed_size = 100\n",
    "\n",
    "x_total = x_train + x_test\n",
    "x_total = [review.split() for review in x_total]\n",
    "\n",
    "embedding_model = FastText(\n",
    "    x_total, \n",
    "    size=embed_size,\n",
    "    window=5, \n",
    "    min_count=1,\n",
    "    min_n=2,\n",
    "    workers=4, \n",
    "    sg=1, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9f3dcf8976ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'영화'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36minit_sims\u001b[0;34m(self, replace)\u001b[0m\n\u001b[1;32m   2016\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams_norm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_model.wv.most_similar('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sent in x_train:\n",
    "    temp_cbow = [embedding_model.wv.__getitem__(token) if embedding_model.wv.__contains__(token) else np.zeros((1,embed_size)) for token in sent]\n",
    "    X_train.append(np.sum(temp_cbow, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sent in x_test:\n",
    "    temp_cbow = [embedding_model.wv.__getitem__(token) if embedding_model.wv.__contains__(token) else np.zeros((1,embed_size)) for token in sent]\n",
    "    X_test.append(np.sum(temp_cbow, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y 더미화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "l2i_dict = defaultdict(lambda : len(l2i_dict))\n",
    "\n",
    "y_train = [l2i_dict[y] for y in y_train]\n",
    "y_test = [l2i_dict[y] for y in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7477648658919536\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'sag',\n",
    "                         multi_class = 'multinomial')\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Variable로 변환\n",
    "def convert_to_long_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Variable로 변환\n",
    "def convert_to_float_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.FloatTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_to_float_variable(X_train)\n",
    "X_test = convert_to_float_variable(X_test)\n",
    "\n",
    "y_train = convert_to_long_variable(y_train)\n",
    "y_test = convert_to_long_variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hid_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hid_size = hid_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, hid_size), nn.Tanh(), nn.Dropout(),\n",
    "            nn.Linear(hid_size, hid_size), nn.Tanh(), nn.Dropout(),\n",
    "            nn.Linear(hid_size, hid_size), nn.Tanh(), nn.Dropout(),\n",
    "            nn.Linear(hid_size, hid_size), nn.Tanh(), nn.Dropout(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )    \n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logit = self.mlp(x)\n",
    "        sigmoid = torch.sigmoid(logit)\n",
    "        return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = embed_size\n",
    "HID_SIZE = 100\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "model = MLP(EMBED_SIZE, HID_SIZE, DROPOUT)\n",
    "\n",
    "lr = 0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Dropout(p=0.5)\n",
       "    (9): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (10): Tanh()\n",
       "    (11): Dropout(p=0.5)\n",
       "    (12): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 0.08104757231473923,  accuracy :0.748\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 0.07757835146784782,  accuracy :0.741\n",
      "=================================================================================================\n",
      "Train epoch : 3,  loss : 0.07705132654309273,  accuracy :0.712\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 0.07629942080378532,  accuracy :0.773\n",
      "=================================================================================================\n",
      "Train epoch : 5,  loss : 0.07631950217485428,  accuracy :0.733\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 5, Test Loss : 0.509 , Test Accuracy : 0.744\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 6,  loss : 0.07254600870609283,  accuracy :0.759\n",
      "=================================================================================================\n",
      "Train epoch : 7,  loss : 0.07138375705480575,  accuracy :0.780\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 0.07097809028625489,  accuracy :0.764\n",
      "=================================================================================================\n",
      "Train epoch : 9,  loss : 0.07041480082273484,  accuracy :0.776\n",
      "=================================================================================================\n",
      "Train epoch : 10,  loss : 0.06955311509966851,  accuracy :0.767\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 10, Test Loss : 0.484 , Test Accuracy : 0.761\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 11,  loss : 0.06886974853277207,  accuracy :0.770\n",
      "=================================================================================================\n",
      "Train epoch : 12,  loss : 0.06870384353399277,  accuracy :0.774\n",
      "=================================================================================================\n",
      "Train epoch : 13,  loss : 0.06807483130693436,  accuracy :0.784\n",
      "=================================================================================================\n",
      "Train epoch : 14,  loss : 0.06744951891899109,  accuracy :0.778\n",
      "=================================================================================================\n",
      "Train epoch : 15,  loss : 0.06709379875659943,  accuracy :0.791\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 15, Test Loss : 0.476 , Test Accuracy : 0.775\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 16,  loss : 0.06698055163025857,  accuracy :0.778\n",
      "=================================================================================================\n",
      "Train epoch : 17,  loss : 0.06620017445087432,  accuracy :0.796\n",
      "=================================================================================================\n",
      "Train epoch : 18,  loss : 0.06583752653002739,  accuracy :0.770\n",
      "=================================================================================================\n",
      "Train epoch : 19,  loss : 0.06597154286503792,  accuracy :0.791\n",
      "=================================================================================================\n",
      "Train epoch : 20,  loss : 0.06604600039124489,  accuracy :0.801\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 20, Test Loss : 0.467 , Test Accuracy : 0.773\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 21,  loss : 0.06552775582671165,  accuracy :0.788\n",
      "=================================================================================================\n",
      "Train epoch : 22,  loss : 0.06498881775140762,  accuracy :0.797\n",
      "=================================================================================================\n",
      "Train epoch : 23,  loss : 0.06503838849067688,  accuracy :0.778\n",
      "=================================================================================================\n",
      "Train epoch : 24,  loss : 0.06485675504803658,  accuracy :0.789\n",
      "=================================================================================================\n",
      "Train epoch : 25,  loss : 0.0650423219203949,  accuracy :0.814\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 25, Test Loss : 0.465 , Test Accuracy : 0.779\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "epochs = 25\n",
    "train_size = X_train.size(0)\n",
    "train_idx = np.arange(train_size)\n",
    "\n",
    "val_size = 10000\n",
    "val_idx = np.arange(val_size)\n",
    "\n",
    "batch_size = 1000\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # 학습 데이터 셔플링\n",
    "    random.shuffle(train_idx)\n",
    "    X_train = X_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for start_idx, end_idx in zip(range(0, train_size, batch_size),\n",
    "                                  range(batch_size, train_size +1, batch_size)):\n",
    "        \n",
    "        X_batch = X_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx]\n",
    "        \n",
    "        sigmoid = model(X_batch)\n",
    "        predict = sigmoid.ge(0.5).squeeze(1).long()\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(sigmoid.squeeze(1), y_batch.float())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.eval()\n",
    "        X_val = X_test[val_idx]\n",
    "        y_val = y_test[val_idx]\n",
    "        \n",
    "        sigmoid = model(X_val)\n",
    "        predict = sigmoid.ge(0.5).squeeze(1).long()\n",
    "        \n",
    "        acc = (predict == y_val).sum().item() / val_size\n",
    "        loss = criterion(sigmoid.squeeze(1), y_val.float())\n",
    "\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item(), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
