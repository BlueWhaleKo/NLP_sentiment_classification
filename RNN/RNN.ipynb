{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# data를 읽어옴\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = read_txt('../ratings_train.txt')\n",
    "x_test, y_test = read_txt('../ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x.split() for x in x_train]\n",
    "x_test = [x.split() for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙..', '진짜', '짜증나네요', '목소리']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 75173, 1: 74827})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOS EOS 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_token(token_ls):\n",
    "    sos = '<SOS>'\n",
    "    eos = '<EOS>'\n",
    "    \n",
    "    for tokens in token_ls:\n",
    "        tokens = [sos] + tokens + [eos] # text\n",
    "        yield tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = defaultdict(lambda : len(token2idx))\n",
    "pad = token2idx['<PAD>'] #0\n",
    "sos = token2idx['<SOS>'] #1\n",
    "eos = token2idx['<EOS>'] #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(add_start_end_token(x_train))\n",
    "x_test = list(add_start_end_token(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>', '아', '더빙..', '진짜', '짜증나네요', '목소리', '<EOS>']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length를 맞추기 위한 padding\n",
    "def add_padding(token_ls, max_len):\n",
    "    pad = '<PAD>'\n",
    "    seq_length_ls = []\n",
    "    \n",
    "    for i, tokens in enumerate(token_ls):\n",
    "        seq_length = len(tokens)\n",
    "        \n",
    "        # 짧으면 padding을 추가\n",
    "        if seq_length < max_len:\n",
    "            seq_length_ls.append(seq_length)\n",
    "            token_ls[i] += [pad] * (max_len - seq_length)\n",
    "        \n",
    "        # 길이가 길면, max_len까지의 token만 사용\n",
    "        elif seq_length >= max_len:\n",
    "            seq_length_ls.append(max_len)\n",
    "            token_ls[i] = tokens[:max_len]\n",
    "            \n",
    "    return token_ls, seq_length_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 30\n",
    "x_train, x_train_seq_length = add_padding(x_train, max_sequence_length)\n",
    "x_test, x_test_seq_length = add_padding(x_test, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting token to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 idx 부여\n",
    "def convert_token_to_idx(token_ls):\n",
    "     \n",
    "    for tokens in token_ls:\n",
    "        yield [token2idx[token] for token in tokens]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(convert_token_to_idx(x_train))\n",
    "x_test = list(convert_token_to_idx(x_test))\n",
    "\n",
    "idx2token = {val : key for key,val in token2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> 아 더빙.. 진짜 짜증나네요 목소리 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2token[x] for x in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting by sequence_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_sequence_length(x, y, seq_len):\n",
    "    sorted_idx = np.argsort(seq_len)[::-1]\n",
    "    \n",
    "    x = np.array(x)[sorted_idx]\n",
    "    y = np.array(y)[sorted_idx]\n",
    "    seq_len = np.array(seq_len)[sorted_idx]\n",
    "    \n",
    "    return x, y, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_train_seq_length = sort_by_sequence_length(x_train, y_train, x_train_seq_length)\n",
    "x_test, y_test, x_test_seq_length = sort_by_sequence_length(x_test, y_test, x_test_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     1,    540,    298, 297657,  13552,   2639, 297658, 297659,\n",
       "       297660, 297661,  21755, 297662,  80473,    715,    141, 297663,\n",
       "        31873, 276536,   8747, 196553,   9286, 297664, 151635,     84,\n",
       "          141, 297665,    707,    581, 297666, 297667])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq_length[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Variable로 변환\n",
    "def convert_to_variable(x):\n",
    "    return Variable(torch.LongTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_variable(x_train)\n",
    "x_test = convert_to_variable(x_test)\n",
    "\n",
    "y_train = convert_to_variable(y_train)\n",
    "y_test = convert_to_variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " '솔직히',\n",
       " '초반에는',\n",
       " '좀',\n",
       " '아쉽지만',\n",
       " '갈수록',\n",
       " '갑툭튀',\n",
       " '개쩔고',\n",
       " '반전도',\n",
       " '있었고',\n",
       " '갠적으로',\n",
       " '꽤',\n",
       " '괜찮게',\n",
       " '봤는데',\n",
       " '평점이',\n",
       " '이상하다.',\n",
       " '딱7점',\n",
       " '중후반은',\n",
       " '되어야',\n",
       " '되는데,',\n",
       " '7.81이',\n",
       " '적당한',\n",
       " '평점이다.',\n",
       " '그리고',\n",
       " '은주',\n",
       " '첨',\n",
       " '봤을때',\n",
       " '한효주닮아서',\n",
       " '좀',\n",
       " '놀랐다.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2token[x.item()] for x in x_train[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, token2idx, vocab_size, embed_size, hid_size, n_layers, dropout, n_category):\n",
    "        super(RNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.padding_index = token2idx['<PAD>']\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embed_size, \n",
    "            padding_idx=self.padding_index\n",
    "        )\n",
    "        \n",
    "        self.hid_size = hid_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drouput = dropout\n",
    "        self.n_category = n_category\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_size, hid_size, n_layers, batch_first=True)\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hid_size, n_category), nn.Tanh(), nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.outputs = []\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, hid_size(n_neuron))\n",
    "        hidden = Variable(torch.randn(self.n_layers, batch_size, self.hid_size))\n",
    "        return hidden    \n",
    "    \n",
    "    def forward(self, x, x_sequence_length):\n",
    "        # init h randomly\n",
    "        batch_size = x.size(0)\n",
    "        self.h = self.init_hidden(batch_size)\n",
    "        \n",
    "        # embedding\n",
    "        x = self.embed(x) # sequence_length(max_len), batch_size, embed_size\n",
    "        \n",
    "        # packing for rnn\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, x_sequence_length, batch_first=True)\n",
    "        \n",
    "        # RNN\n",
    "        output, self.h = self.rnn(x, self.h)\n",
    "        \n",
    "        # unpack\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # cbow\n",
    "        x = x.sum(dim = 1)\n",
    "        \n",
    "        # fully-connect\n",
    "        logit = self.lin(x)\n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'token2idx' : token2idx,\n",
    "    'vocab_size' : len(token2idx),\n",
    "    'embed_size' : 64,\n",
    "    'hid_size' : 64,\n",
    "    'n_layers' : 2,\n",
    "    'dropout' : 0.5,\n",
    "    'n_category' : 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(448188, 64, padding_idx=0)\n",
       "  (rnn): RNN(64, 64, num_layers=2, batch_first=True)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr=0.001, lr_decay_epoch=10):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to %s'%(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 3.963375,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 3.9973619384765624,  accuracy :0.860\n",
      "=================================================================================================\n",
      "Train epoch : 3,  loss : 4.0389154296875,  accuracy :0.867\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 3.9888525146484377,  accuracy :0.864\n",
      "=================================================================================================\n",
      "Train epoch : 5,  loss : 3.96855693359375,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 6,  loss : 3.9455397216796877,  accuracy :0.862\n",
      "=================================================================================================\n",
      "Train epoch : 7,  loss : 3.94292861328125,  accuracy :0.865\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 3.9366978515625,  accuracy :0.863\n",
      "=================================================================================================\n",
      "Train epoch : 9,  loss : 3.9222626220703125,  accuracy :0.868\n",
      "=================================================================================================\n",
      "Train epoch : 10,  loss : 3.9387919677734375,  accuracy :0.864\n",
      "=================================================================================================\n",
      "LR is set to 0.001\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 10, Test Loss : 0.580 , Test Accuracy : 0.756\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 11,  loss : 3.8849404296875,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 12,  loss : 3.8680816162109375,  accuracy :0.858\n",
      "=================================================================================================\n",
      "Train epoch : 13,  loss : 3.8687645263671877,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 14,  loss : 3.8437479736328126,  accuracy :0.867\n",
      "=================================================================================================\n",
      "Train epoch : 15,  loss : 3.8583749267578127,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 16,  loss : 3.8531617919921874,  accuracy :0.868\n",
      "=================================================================================================\n",
      "Train epoch : 17,  loss : 3.8392936767578125,  accuracy :0.866\n",
      "=================================================================================================\n",
      "Train epoch : 18,  loss : 3.83793779296875,  accuracy :0.867\n",
      "=================================================================================================\n",
      "Train epoch : 19,  loss : 3.847011083984375,  accuracy :0.867\n",
      "=================================================================================================\n",
      "Train epoch : 20,  loss : 3.832303662109375,  accuracy :0.869\n",
      "=================================================================================================\n",
      "LR is set to 0.00010000000000000002\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 20, Test Loss : 0.574 , Test Accuracy : 0.758\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 21,  loss : 3.8315287109375,  accuracy :0.869\n",
      "=================================================================================================\n",
      "Train epoch : 22,  loss : 3.85181044921875,  accuracy :0.868\n",
      "=================================================================================================\n",
      "Train epoch : 23,  loss : 3.8299896484375,  accuracy :0.865\n",
      "=================================================================================================\n",
      "Train epoch : 24,  loss : 3.8472972900390623,  accuracy :0.865\n",
      "=================================================================================================\n",
      "Train epoch : 25,  loss : 3.85802421875,  accuracy :0.864\n",
      "=================================================================================================\n",
      "Train epoch : 26,  loss : 3.8372686279296877,  accuracy :0.868\n",
      "=================================================================================================\n",
      "Train epoch : 27,  loss : 3.8589898193359375,  accuracy :0.865\n",
      "=================================================================================================\n",
      "Train epoch : 28,  loss : 3.8398020751953124,  accuracy :0.868\n",
      "=================================================================================================\n",
      "Train epoch : 29,  loss : 3.8602102294921874,  accuracy :0.865\n",
      "=================================================================================================\n",
      "Train epoch : 30,  loss : 3.839629736328125,  accuracy :0.869\n",
      "=================================================================================================\n",
      "LR is set to 1.0000000000000003e-05\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 30, Test Loss : 0.575 , Test Accuracy : 0.757\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 31,  loss : 3.839428369140625,  accuracy :0.872\n",
      "=================================================================================================\n",
      "Train epoch : 32,  loss : 3.8417737060546875,  accuracy :0.868\n",
      "=================================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-e4fb74d141bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lr = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    '''\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    x_train_seq_length = x_train_seq_length[train_idx]\n",
    "    '''\n",
    "    \n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        \n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        x_batch_seq_length = x_train_seq_length[start_idx: end_idx]\n",
    "        \n",
    "        scores = model(x_batch, x_batch_seq_length)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    optimizer = adjust_learning_rate(optimizer, epoch, lr, 10) # adjust learning_rate while training\n",
    "    \n",
    "    if (epoch) % 10 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test, x_test_seq_length)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / y_test.size(0)\n",
    "        loss = criterion(scores, y_test.long())\n",
    "        \n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch, loss.item()/y_test.size(0), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
